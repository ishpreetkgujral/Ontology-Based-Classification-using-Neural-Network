# -*- coding: utf-8 -*-
"""Major_Project_Ontology_Based_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ckSXQT5nwhyUU1eirgQ7m5Q5myFvMXWf
"""

# necessary imports
import pandas as pd
import numpy as np
from collections import defaultdict
import csv,nltk
# nltk.download()
!python -m nltk.downloader all
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk import  pos_tag
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from operator import itemgetter, attrgetter

from sklearn.feature_extraction.text import CountVectorizer
from numpy import array
import string
import random
! pip install tensorflow-gpu
! pip install keras
from math import floor
from collections import OrderedDict
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

import tensorflow
import keras
from keras import models
from keras import layers
from tensorflow.python.keras.layers import Activation, Dense
from tensorflow.python.keras.models import Sequential


# this function helps in dataset creation by creating a disease-symptom dictionary
def dataset_creation():
    disease_symptom_dict = defaultdict(list)
    # reader = csv.reader(codecs.open('/content/Final_Dataset.csv', 'rU', 'utf-16-le'))
    # data=list(reader)
    with open('/content/Final_Dataset.csv','r',encoding='utf-8') as csvFile:
      reader=csv.reader(csvFile)
      data=list(reader)
    csvFile.close()
    keywords=[]
    # print(len(data))
    for i in range(1,len(data)):
      # print(i,'Entity:'+data[i][0])
      data[i][0].replace('\xa0',' ')
      # print('Keywords:'+data[i][1])
      # print('Additional Keywords:'+data[i][2])
      for j in range(0,len(data[i][1])):
        data[i][1][j].replace('\xa0',' ')
      keywords=data[i][1].split(',')
      # print(keywords)
      disease_symptom_dict[str(data[i][0]).strip()].append(keywords)
    # print(len(disease_symptom_dict))
    x = []
    y = []
    for i in disease_symptom_dict:
        y.append(i)
        x.append(disease_symptom_dict[i][0])
    temp = []
    for i in x:
        for j in i:
            temp.append(j)

    from collections import OrderedDict
    temp = list(OrderedDict((x, True) for x in temp).keys())
    for i in range(len(y)):
      for j in range(150):
          y.append(y[i])
          x.append(random.sample(x[i],floor(len(x[i])/3)))
    x_ = []
    for i in x:
      x_.append(' '.join(i))
    return x_,y

# now data_x,data_y contains the diseases and their respective symptoms 
data_x,data_y=dataset_creation()

# this class performs cleaning of the data which includes-
# tokenization,stopwords removal,lemmatization,POS Tagging, Stemming and providing priority
# the priority is provided according to the frequency as well as lemmatized result of the word

class cleaning_nlp_processes:

    def __init__(self,description):
        self.Definition=description

    def same_frequency_words_priority(self,word_list):
        final_list=[]
        parts_of_speech = self.parts_of_speech()
        for i in range(0,len(word_list)):
            words = self.lemmatization(parts_of_speech, [word_list[i][0]])
            if len(words)==0 or word_list[i][0]==words[0]:
                # print(words,word_list[i],"Flag=1")
                final_list.append((word_list[i][0],word_list[i][1],1))
            else:
                # print(words,word_list[i]," Flag=0")
                final_list.append((word_list[i][0],word_list[i][1],0))
        return final_list


    def priority_provide(self, stop_words_remove,freq_dict):
        final_priority_list=[]
        # print(freq_dict)
        import operator
        sorted_x = sorted(freq_dict.items(), key=operator.itemgetter(1),reverse=True)
        import collections
        sorted_dict = dict(collections.OrderedDict(sorted_x))
        sorted_list = [(k, v) for k, v in sorted_dict.items()]
        # print(sorted_list)
        splice_index=[]
        for i in range(1,len(sorted_list)):
            if sorted_list[i-1][1]==sorted_list[i][1]:
                continue
            else:
                splice_index.append(i)
        # print(splice_index)
        if len(splice_index)==0:
            final_priority_list=self.same_frequency_words_priority(sorted_list)

        elif len(splice_index)==1:
            list_1=(sorted_list[:splice_index[0]])
            list_2=(sorted_list[splice_index[0]:])
            final_priority_list = self.same_frequency_words_priority(list_1)
            final_priority_list += self.same_frequency_words_priority(list_2)

        else:
            list_1=(sorted_list[:splice_index[0]])
            final_priority_list=self.same_frequency_words_priority(list_1)

            for i in range(1,len(splice_index)):
                list_i=(sorted_list[splice_index[i-1]:splice_index[i]])
                final_priority_list+=self.same_frequency_words_priority(list_i)
            list_end=(sorted_list[splice_index[len(splice_index)-1]:])
            final_priority_list += self.same_frequency_words_priority(list_end)

        # print(final_priority_list)
        sorted_x = sorted(freq_dict.items(), key=operator.itemgetter(1), reverse=True)
        return final_priority_list

    def keywords_formation(self):
        word_arr = self.tokenizing([self.Definition])
        stopwords_remove = self.stop_words_removal(word_arr)
        # print(stopwords_remove)
        count = self.freq(stopwords_remove)
        final_prority_array=self.priority_provide(stopwords_remove,count)
        # print(final_prority_array)
        return final_prority_array

    def tokenizing(self, parameters):
        # Tokenising..............................................
        data = '';
        for i in parameters:
            data += i + ' '
        data = data.strip()
        # print(data)
        data = data.split(' ')
        # print(data)
        sentence = ''
        for i in data:
            sentence += i+' '
        sentence=sentence.strip()
        word_arr = word_tokenize(sentence.lower())
        #  Here lower case is done to remove more stopwords, but some information is lost
        # print( word_arr)
        return word_arr

    def stop_words_removal(self, word_arr):
        # Stopwords and Punctuations.........................................
        stop_words = stopwords.words('english')
        # Stop Words are present of different languages, for papers of different languages.
        # Cleaning words(removing stopwords and punctuations
        # print(stop_words)
        punctuations = list(string.punctuation)
        # print(string.punctuation)
        stop_words += punctuations
        # print(len(stop_words))
        file_data = []
        with open('stopwords', 'r') as f:
            for line in f:
                for word in line.split():
                    file_data.append(word)
        stop_words += file_data
        stop_words = set(stop_words)
        stop_words = list(stop_words)
        new_word_arr = []
        for i in word_arr:
            new_word_arr.append(i.lower())
        clean_words = [w for w in new_word_arr if not w in stop_words]
        # print(len(stop_words))
        # print("After cleaning the stopwords, no of words:", len(clean_words))
        # print(clean_words)
        return clean_words

    def freq(self, words):
        dict = {
        }
        for i in words:
            dict[i] = words.count(i)
        # print("Dictionary after stopwords removal(no lemmatization)")
        # print(dict)
        return dict

    def lemmatization(self, part_of_speech, clean_words):
        lemmatized = []
        lemmatizer = WordNetLemmatizer()
        # print pos[2][0]
        # print clean_words[1]
        for i in range(0, len(part_of_speech)):
            for j in range(0, len(clean_words)):
                if part_of_speech[i][0].lower() == clean_words[j]:
                    # print part_of_speech[i][0], clean_words[j], part_of_speech[i][1]
                    lemmatized.append(
                        (lemmatizer.lemmatize(clean_words[j], pos=self.pos_to_wordnet(part_of_speech[i][1]))).lower())
                    break
                else:
                    continue
        # print("After Lemmatization No. of Words:", len(list(set(lemmatized))))
        # print(lemmatized)
        return list(set(lemmatized))

    def parts_of_speech(self):
        parts_of_speech = pos_tag(word_tokenize(self.Definition))  # +' '+self.synonym+ ' ' + self.name))
        # print(parts_of_speech)
        return parts_of_speech

    def pos_to_wordnet(self, pos_tag):
        if pos_tag.startswith('J'):
            return wordnet.ADJ
        elif pos_tag.startswith('N'):
            return wordnet.NOUN
        elif pos_tag.startswith('V'):
            return wordnet.VERB
        # elif pos_tag.startswith('M'):
        #     return wordnet.MODAL
        # elif pos_tag.startswith('R'):
        #     return wordnet.ADVERB
        else:
            return wordnet.NOUN

    def stemming(self, cleaning_words):
        '''
        The process of stemmimng is very dumb. Not always give reslutant output. The information passed through it might result in bad output.
        Stemming is the process of finding the root word of the given word.
        Prefer Lemmatization over it.
        '''
        ps = PorterStemmer()
        # stem_words = ['play', 'playing', 'played', 'player', "happy", 'happier']
        stemmed_words = [ps.stem(w) for w in cleaning_words]
        # print(len(stemmed_words))
        # print(stemmed_words)
        return stemmed_words

# imports to load csv
import csv
import codecs

csvReader = csv.reader(codecs.open('/content/Final_Dataset.csv', 'rU', 'utf-16'))


# this class helps to clean the data obtained from the ontology

class keywords_from_ontology:
    def __init__(self,properties):
        # self.defintion=definition
        # self.synonym=synonym
        # self.entity_name=entity_name
        self.properties=properties

    def keywords_creation(self):
        tokens=self.tokenizing(self.properties)
        clean_words=self.stop_words_removal(tokens)
        return clean_words

    def stop_words_removal(self, word_arr):
        # Stopwords and Punctuations.........................................
        stop_words = stopwords.words('english')
        # Stop Words are present of different languages, for papers of different languages.
        # Cleaning words(removing stopwords and punctuations
        # print(stop_words)
        
        punctuations = list(string.punctuation)
        # print(string.punctuation)
        stop_words += punctuations
        # print(len(stop_words))
        file_data = []
        with open('stopwords', 'r') as f:
            for line in f:
                for word in line.split():
                    file_data.append(word)
        stop_words += file_data
        stop_words = set(stop_words)
        stop_words = list(stop_words)
        new_word_arr = []
        for i in word_arr:
            new_word_arr.append(i.lower())
        clean_words = [w for w in new_word_arr if not w in stop_words]
        # print(len(stop_words))
        # print("After cleaning the stopwords, no of words:", len(clean_words))
        # print(clean_words)
        return clean_words

    def tokenizing(self, parameters):
        # Tokenising..............................................
        data = '';
        for i in parameters:
            data += i + ' '
        data = data.strip()
        # print(data)
        data = data.split(' ')
        # print(data)
        sentence = ''
        for i in data:
            sentence += i+' '
        sentence=sentence.strip()
        word_arr = word_tokenize(sentence.lower())
        #  Here lower case is done to remove more stopwords, but some information is lost
        # print( word_arr)
        return word_arr

# a node is created containing the disease and their respective keywords which are the properties of the diseases

class Node:
    def __init__(self,entity,keywords):
        self.entity=entity
        # self.children=children
        self.keywords=keywords
        # self.deprecated=deprecated

# this function creates the ontology tree
def ontology_creation():
    with open('DOID_Ontology - Sheet1.csv', 'r') as csvFile:
        reader = csv.reader(csvFile)
        data = list(reader)
    csvFile.close()
    # for i in range(0,len(data[0])):
    #   print(i,data[0][i])
    # print(additional_keywords[0])
    temp_object_for_keywords = keywords_from_ontology('')
    nodes=[]
    for k in range(1,len(data)):
      parent_keywords=[]
      child_keywords=[]
      temp_object_for_keywords = keywords_from_ontology([str(data[k][0]), str(data[k][3]),
                                                               str(data[k][4]),str(data[k][5]),
                                                               str(data[k][6]), str(data[k][7]),
                                                               str(data[k][7]),str(data[k][9]), 
                                                               str(data[k][10]),str(data[k][13]),
                                                               str(data[k][14]),str(data[k][24]),
                                                               str(data[k][25]),str(data[k][54]),
                                                               str(data[k][55]),str(data[k][58])])      
      for i in range(1,len(data)):
        if data[i][0]==data[k][2]:
          temp_object_for_keywords_parent = keywords_from_ontology([str(data[i][0]), str(data[i][3]),
                                                               str(data[i][4]),str(data[i][5]),
                                                               str(data[i][6]), str(data[i][7]),
                                                               str(data[i][8]),str(data[i][9]), 
                                                               str(data[i][10]),str(data[i][13]),
                                                               str(data[i][14]),str(data[i][24]),
                                                               str(data[i][25]),str(data[i][54]),
                                                               str(data[i][55]),str(data[i][58])])
          parent_keywords=list(set(temp_object_for_keywords_parent.keywords_creation()))
          break
        else:
          continue

      child_keywords=list(set(temp_object_for_keywords.keywords_creation()))
      obj=Node(str(data[k][0]),list(set(child_keywords+parent_keywords)))
      nodes.append(obj)
    
    return nodes

# this function matches the ontology properties with the data obtained from the Final_Dataset.csv

def ontology_mathing(tree,keywords):
    # print('Keywords formed from given x data:',keywords)
    # print()
    nodes_to_search=[]
    found_nodes = []
    priority_1 = []
    priority_2 = []
    priority_3 = []
    
    for i in range(0, len(keywords)):
        if keywords[i][1] != 1:
            priority_1.append(keywords[i][0])
        else:
            if keywords[i][2] == 1:
                priority_2.append(keywords[i][0])
            else:
                priority_3.append(keywords[i][0])

    # nodes_to_search.append(tree[0])
    # nodes_to_search.append(tree[76])
    # i=0
    # flag=0
    # while i in range(0,len(nodes_to_search)):
    #     if i==0:
    #         node=nodes_to_search.index(tree[0])
    #         print('Entity whose children are Currently Under Search:',tree[node].entity)
    #         flag=0
    #     else:
    #         node = tree.index(nodes_to_search[i])
    #         print('Entity whose children are Currently Under Search:', tree[node].entity)
    #         flag=1
    #     for j in range(0,len(tree[node].children)):
    #         print('Child under Consideration:',tree[node].children[j])
    #         for k in range(i,len(tree)):
    #             if tree[k].entity==tree[node].children[j]:
    #                 print('Found Entity in Tree:', tree[k].entity)
    #                 if flag==0:
    #                     for l in range(0,len(keywords)):
    #                         if keywords[l][0] in tree[k].keywords:
    #                             found_nodes.append((k,tree[k].entity,keywords[l][0]))
    #                             nodes_to_search.append(tree[k])
    #                             print('Keyword ',keywords[l][0],' Found in Node:',tree[k].entity)
    #                             break
    #                         else:
    #                             continue
    #                 else:
    #                     nodes_to_search.append(tree[k])
    #                     for l in range(0,len(keywords)):
    #                         if keywords[l][0] in tree[k].keywords:
    #                             found_nodes.append((k,tree[k].entity,keywords[l][0]))
    #                             print('Keyword ',keywords[l][0],' Found in Node:',tree[k].entity)
    #                             break
    #                         else:
    #                             continue
    #             else:
    #                 continue
    #     i=i+1
        # print(nodes_to_search)
    priority_1_count = 0
    priority_2_count = 0
    priority_3_count = 0

    for i in range(0,len(tree)):
        priority_1_count = 0
        priority_2_count = 0
        priority_3_count = 0
        for j in range(0,len(priority_1)):
                if priority_1[j].lower() in [x.lower() for x in tree[i].keywords]:
                    priority_1_count=priority_1_count+1
                else:
                    continue
        for j in range(0, len(priority_2)):
            if priority_2[j].lower() in [x.lower() for x in tree[i].keywords]:
                priority_2_count = priority_2_count + 1
            else:
                continue

        for j in range(0, len(priority_3)):
            if priority_3[j].lower() in [x.lower() for x in tree[i].keywords]:
                priority_3_count = priority_3_count + 1
            else:
                continue

        # print("Priority_1:", priority_1_count)
        # print("Priority_2:", priority_2_count)
        # print("Priority_3:", priority_3_count)

        # if priority_1_count>=1:
        #     if priority_2_count>=1:
        #         if priority_3_count>=1:
        #             found_nodes.append((tree[i].entity))
        #         else:
        #             continue
        #     else:
        #         continue
        # else:
        #     continue
        if priority_1_count+priority_2_count+priority_3_count>=1:
            # print('Possible Node :', tree[i].entity)
            # print('Possible Node Keywords:', tree[i].keywords)
            # print('Keywords Passed:',keywords)
            found_nodes.append((tree[i].entity))
        else:
            continue

    # for i in range(0,len(found_nodes)):
    #     print('Possible Class/Node:',found_nodes[i])
    classes=[]
    for i in range(0,len(found_nodes)):
        classes.append(found_nodes[i])
    classes=list(set(classes))
    # print(classes)
    return classes

# this is for classification without using ontology
def Classification_without_Ontology_Neural_Network(x_train, y_train,x_test,y_test):
    from sklearn.preprocessing import LabelEncoder, OneHotEncoder
    le = LabelEncoder()
    output_data_complete= le.fit_transform(data_y)
    output_data=le.transform(y_train)
    output_data_test=le.transform(y_test)
    # print(output_data.shape)
    # print(le.transform(le.classes_))
    # print(output_data.reshape(-1,1))
    onehotencoder = OneHotEncoder()
    output_data_complete=onehotencoder.fit_transform(output_data_complete.reshape(-1,1))
    output_data=onehotencoder.transform(output_data.reshape(-1,1))
    output_data_test=onehotencoder.transform(output_data_test.reshape(-1,1))
    
    print('Neural Network is being executed......')
    count_vector = CountVectorizer(max_features=300)  
    training_data = count_vector.fit_transform(x_train)
    predictions=[]
    testing_data = count_vector.transform(x_test)
    model = Sequential()
    layer1 = Dense(units= 300, activation = 'relu', input_dim = 300)                            # Creating a Dense Layer
    model.add(layer1)                                                                           # Adding layer to model.
    model.add(Dense(units=32, activation = 'relu'))
    model.add(Dense(units=32, activation = 'relu'))
    model.add(Dense(units=32, activation = 'relu'))
    model.add(Dense(units=130, activation = 'sigmoid'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model.fit(training_data.todense(), output_data.toarray(), epochs=20, batch_size = 50, validation_data=(testing_data.todense(), output_data_test.toarray()))
    predictions = model.predict(testing_data.todense())
    # print(predictions)
    actual_predictions=[]
    for i in predictions:
      # print(i.shape)
      actual_predictions.append((str(le.inverse_transform(onehotencoder.inverse_transform([i]))[0])))
    return actual_predictions

def entire_process_without_ontology_neural_network():
    data_x,data_y=dataset_creation()
    # print(output_data)
    # # data_x=np.array(data_x).reshape(-1, 1)
    # print(output_data[0].toarray())
    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2,random_state=0)
    predictions_without_ontology = Classification_without_Ontology_Neural_Network(x_train, y_train,x_test[:500],y_test[:500])
    print(predictions_without_ontology)
    from sklearn.metrics import classification_report
    print((classification_report(y_test[:len(predictions_without_ontology)], predictions_without_ontology)))
    from sklearn.metrics import precision_recall_fscore_support
    print(precision_recall_fscore_support(y_test[:len(predictions_without_ontology)], predictions_without_ontology, average='macro'))
    # for i in range(0,len(predictions_without_ontology)):
  #   print('Classifier under consideration: '+ 'ANN')
  # print('Accuracy score:',format(accuracy_score(y_test[:len(predictions_without_ontology[i])], predictions_without_ontology[i])))
  #     print('Precision score:',format(precision_score(y_test[:len(predictions_without_ontology[i])], predictions_without_ontology[i], average='micro')))
  #     print('Recall score:',format(recall_score(y_test[:len(predictions_without_ontology[i])], predictions_without_ontology[i], average='micro')))
  #     print('F1 score:',format(f1_score(y_test[:len(predictions_without_ontology[i])], predictions_without_ontology[i], average='micro')))

entire_process_without_ontology_neural_network()

# this is for classification with ontology
def Classification_with_Ontology_Neural_Network(x_train, y_train,x_test,y_test):
    ontology_tree=ontology_creation()
    tree=ontology_tree
    #predictions=[[],[],[],[],[],[],[],[]]
    predictions=[[]]
    c1,c2,c3=0,0,0
    all_classes = list(set(list(set(y_train))+list(set(y_test))))
    # names=['Multinomial Naive Bayes','Bagging', 'KNN', 'SVM','Decision Tree','Random Forest','Logistic Regression','AdaBoost'] #'Gradient Boost']
    names=['ANN']
    from sklearn.preprocessing import LabelEncoder, OneHotEncoder
    le = LabelEncoder()
    output_data_complete= le.fit_transform(data_y)
    output_data=le.transform(y_train)
    # print(output_data.shape)
    # print(le.transform(le.classes_))
    # print(output_data.reshape(-1,1))
    onehotencoder = OneHotEncoder()
    output_data_complete=onehotencoder.fit_transform(output_data_complete.reshape(-1,1))
    output_data=onehotencoder.fit_transform(output_data.reshape(-1,1))
    
    # print(output_data)
    # # data_x=np.array(data_x).reshape(-1, 1)
    # print(output_data[0].toarray())
    keywords_for_matching=[]
    
    with open('Final_Dataset.csv', 'r') as csvFile:
        reader = csv.reader(csvFile)
        additional_keywords = list(reader)
    csvFile.close()
    
    for i in range(1,len(additional_keywords)):
      obj = cleaning_nlp_processes(additional_keywords[i][2]+additional_keywords[i][3])
      additional_words = obj.keywords_formation()
      # print('Keywords From Dataset for '+additional_keywords[i][0] +' are:')
      # print(additional_words)
      keywords_for_matching.append((additional_keywords[i][0],additional_words))

    count_vector_entire_data_set = CountVectorizer(max_features=300)
    training_data_entire_data_set = count_vector_entire_data_set.fit_transform(x_train)
    model_entire_data_set = Sequential()
    layer1_entire_data_set = Dense(units= 300, activation = 'relu', input_dim = 300)                            # Creating a Dense Layer
    model_entire_data_set.add(layer1_entire_data_set)                                                                           # Adding layer to model.
    model_entire_data_set.add(Dense(units=32, activation = 'relu'))
    model_entire_data_set.add(Dense(units=32, activation = 'relu'))
    model_entire_data_set.add(Dense(units=32, activation = 'relu'))
    model_entire_data_set.add(Dense(units=130, activation = 'sigmoid'))
    model_entire_data_set.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    
    classifiers_entire_data_set=[model_entire_data_set]

    model = Sequential()
    layer1 = Dense(units= 300, activation = 'relu', input_dim = 300,kernel_initializer='zeros',bias_initializer='zeros')                            # Creating a Dense Layer
    model.add(layer1)                                                                           # Adding layer to model.
    model.add(Dense(units=32, activation = 'relu',kernel_initializer='zeros',bias_initializer='zeros'))
    model.add(Dense(units=32, activation = 'relu',kernel_initializer='zeros',bias_initializer='zeros'))
    model.add(Dense(units=32, activation = 'relu',kernel_initializer='zeros',bias_initializer='zeros'))
    model.add(Dense(units=130, activation = 'sigmoid',kernel_initializer='zeros',bias_initializer='zeros'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
           
    
    # classifiers=[naive_bayes,bagging,KNN,SVM,decision_tree,random_forest,lr,adabost]#,grad_boost]
    classifiers=[model]
    
    for i in range(0,len(classifiers_entire_data_set)):
      # print('Classifier in use:'+names[i])
      classifiers_entire_data_set[i].fit(training_data_entire_data_set.todense(), output_data.toarray(), epochs=20, batch_size = 50)
    # print(count_vector.get_feature_names())
    for z in range(0,len(x_test)):
      if z!=4 or z!=2:
        break
      print('Disease Under Consideration:',y_test[z])
      obj = cleaning_nlp_processes(x_test[z])
      words = obj.keywords_formation()
      print('Initially Keywords from symtoms:')
      print(words)
      for q in range(0,len(keywords_for_matching)):
        # print(y_test[z],keywords_for_matching[q][0],y_test[z].strip()==keywords_for_matching[q][0].strip())
        if y_test[z].strip()==keywords_for_matching[q][0].strip():
          words=words+keywords_for_matching[q][1]
          break
        else:
          continue
      print('Adding additional Keywords From Dataset are:')
      print(words)
      # flag=1
      # for i in range(0,len(tree)):
      #   if tree[i].entity[len(tree[i].entity)-1]=="'":
      #     # print(tree[i].entity[0:len(tree[i].entity)-2].strip(),data_y[no])
      #     if set(word_tokenize(y_test[z])).issubset(word_tokenize(tree[i].entity[0:len(tree[i].entity)-2])):
      #       print('Ontology Node:',tree[i].entity[0:len(tree[i].entity)-2])
      #       print('Keywords of ontology Node:',tree[i].keywords)
      #       flag=0
      #       break
      #     else:
      #       continue
      #   else:      
      #     # print(tree[i].entity,data_y[no])
      #     if set(word_tokenize(y_test[z])).issubset(word_tokenize(tree[i].entity)):
      #       print('Ontology Node:',tree[i].entity)
      #       print('Keywords of ontology Node:',tree[i].keywords)
      #       flag=0
      #       break
      #     else:
      #       continue
      common_classes=[]
      # if flag==1:
        # print('Disease doesn"t occur in ontology')
        # pass
      possible_classes=ontology_mathing(ontology_tree,words)
      print('Possibilities After Ontology Matching:',possible_classes)
      for i in range(0,len(all_classes)):
          dataset_elements=[x.lower() for x in list(set(word_tokenize(all_classes[i])))]
          for j in range(0,len(possible_classes)):
            ontology_node=[x.lower() for x in list(set(word_tokenize(possible_classes[j])))]
            if set(dataset_elements).issubset(set(ontology_node)):
              common_classes.append(all_classes[i])
            else:
              continue      
      common_classes=list(set(common_classes))
      print('Final Possibilites After Ontology Dataset Intersections:',common_classes)
      print('Number of possible Classes after intersection:',len(common_classes))
      print('Does Actual Class exsist in Possible Classes after intersection:', y_test[z] in common_classes)
      if len(common_classes)==0:
        c1=c1+1
        print('No Entry Found in Dataset and Ontology, Using Normal Classification.........')
        print('Comparing Predictions...............')
        print('Actual Value:',y_test[z])
        testing_data = count_vector_entire_data_set.transform([x_test[z]])[0]
        for h in range(0,len(classifiers_entire_data_set)):
          prediction= classifiers_entire_data_set[h].predict(testing_data)[0] 
          predictions[h].append(prediction)
          print('Comparing Predictions...............')
          print('Classifier in use: ',names[h])
          print('Actual value: ',y_test[z])
          print('Predicted Value:',prediction)  
      else:
        c2=c2+1
        print('Using Ontology Matching............')
        indexes_to_use = []
        for j in range(0, len(y_train)):
          if y_train[j] in common_classes:
            indexes_to_use.append(j)
          else:
            continue
        new_x=[x_train[x] for x in indexes_to_use]
        new_y=[y_train[x] for x in indexes_to_use]
        print(new_y)
        final_y=onehotencoder.transform((le.transform(new_y)).reshape(-1,1))
        # for i in new_y:
        #   # print('Class:',i)
        #   # print('Label of Class:',le.transform([i]))
        #   obj=(le.transform([i]).reshape(-1,1))
        #   # print('One Hot encoding of Class:', onehotencoder.transform(obj))
        #   final_y.append((obj))
        # final_y=onehotencoder.transform(final_y)
        # # print(final_y)
        count_vector = CountVectorizer(max_features=400)
        training_data = count_vector.fit_transform(new_x)
        testing_data = count_vector.transform([x_test[z]])[0]
        
        for h in range(0,len(classifiers)):
          classifiers[h].fit(training_data.todense(),final_y.toarray(),epochs=20, batch_size = 50)
          prediction= classifiers[h].predict(testing_data)[0] 
          predictions[h].append(prediction)
          print('Comparing Predictions...............')
          print('Classifier in use: ',names[h])
          print('Actual Value:',y_test[z])
          print('Predicted Value:',prediction)
      #   if set(word_tokenize(y_test[z])).issubset(set(word_tokenize(prediction))):
      #     c3=c3+1
      # print(c1,c2,c3)
      print(((z+1)/len(x_test))*100,'% Completed')
      print()

def entire_process_with_ontology():
    names=['ANN']
    
    data_x,data_y=dataset_creation()
    
    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2,random_state=0)
    predictions_with_ontology = Classification_with_Ontology_Neural_Network(x_train, y_train,x_test[:500],y_test[:500])
    print(predictions_with_ontology)
    
    from sklearn.metrics import classification_report
    
    print((classification_report(y_test[:len(predictions_with_ontology)], predictions_with_ontology)))
    from sklearn.metrics import precision_recall_fscore_support
    print(precision_recall_fscore_support(y_test[:len(predictions_with_ontology)], predictions_with_ontology, average='macro'))
    # metrics=[]
    # for i in range(0,len(predictions_with_ontology)):
    #   classifier_name=names[i]
    #   accuracy_score_with_ontology=format(accuracy_score(y_test[:len(predictions_with_ontology[i])], predictions_with_ontology[i]))
    #   precision_score_with_ontology=format(precision_score(y_test[:len(predictions_with_ontology[i])], predictions_with_ontology[i], average='micro'))
    #   recall_score_with_ontology=format(recall_score(y_test[:len(predictions_with_ontology[i])], predictions_with_ontology[i], average='micro'))
    #   f1_score_with_ontology=format(f1_score(y_test[:len(predictions_with_ontology[i])], predictions_with_ontology[i], average='micro'))
    #   print('Classifier under consideration:'+ classifier_name)
    #   print('Accuracy score:', accuracy_score_with_ontology)
    #   print('Precision score:',precision_score_with_ontology)
    #   print('Recall score:',recall_score_with_ontology)
    #   print('F1 score:',f1_score_with_ontology) 
    #   metrics.append([classifier_name,accuracy_score_with_ontology,precision_score_with_ontology,recall_score_with_ontology,f1_score_with_ontology])
    # return metrics

metrics_with_ontology = entire_process_with_ontology()
metrics_with_ontology

names=['ANN']    
data_x,data_y=dataset_creation()
x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2,random_state=0)

ontology_tree=ontology_creation()
tree=ontology_tree
#predictions=[[],[],[],[],[],[],[],[]]
predictions=[[]]
c1,c2,c3=0,0,0
all_classes = list(set(list(set(y_train))+list(set(y_test))))
names=['ANN']

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
    le = LabelEncoder()
    output_data_complete= le.fit_transform(data_y)
    output_data=le.transform(y_train)
    # print(output_data.shape)
    # print(le.transform(le.classes_))
    # print(output_data.reshape(-1,1))
    onehotencoder = OneHotEncoder()
    output_data_complete=onehotencoder.fit_transform(output_data_complete.reshape(-1,1))
    output_data=onehotencoder.fit_transform(output_data.reshape(-1,1))
    
    # print(output_data)
    # # data_x=np.array(data_x).reshape(-1, 1)
    # print(output_data[0].toarray())
    keywords_for_matching=[]
    

    with open('Final_Dataset.csv', 'r') as csvFile:
        reader = csv.reader(csvFile)
        additional_keywords = list(reader)
    csvFile.close()
    
    for i in range(1,len(additional_keywords)):
      obj = cleaning_nlp_processes(additional_keywords[i][2]+additional_keywords[i][3])
      additional_words = obj.keywords_formation()
      # print('Keywords From Dataset for '+additional_keywords[i][0] +' are:')
      # print(additional_words)
      keywords_for_matching.append((additional_keywords[i][0],additional_words))

    count_vector_entire_data_set = CountVectorizer(max_features=300)
    training_data_entire_data_set = count_vector_entire_data_set.fit_transform(x_train)

# from tensorflow.keras import layers
    # import pickle
    def create_model_entire_data_set(training_data_x,training_data_y,testing_data_x):
      # classifiers=[model]
      model_entire_data_set = Sequential()
      layer1_entire_data_set = Dense(units= 300, activation = 'relu', input_dim = 300)                            # Creating a Dense Layer
      model_entire_data_set.add(layer1_entire_data_set)                                                                           # Adding layer to model.
      model_entire_data_set.add(Dense(units=32, activation = 'relu'))
      model_entire_data_set.add(Dense(units=32, activation = 'relu'))
      model_entire_data_set.add(Dense(units=32, activation = 'relu'))
      model_entire_data_set.add(Dense(units=130, activation = 'sigmoid'))
      model_entire_data_set.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
      # pickle.dump(model_entire_data_set,open('complete_model.sav','wb'))
      classifiers_entire_data_set=[model_entire_data_set]
      for h in range(0,len(classifiers_entire_data_set)):
          prediction= classifiers_entire_data_set[h].predict(testing_data)[0] 
      return prediction

    def create_model(training_data_x,training_data_y,testing_data_x):
      
      model = Sequential()
      layer1 = Dense(units= 300, activation = 'relu', input_dim = 300)                       # Creating a Dense Layer
      model.add(layer1)                                                                           # Adding layer to model.
      model.add(Dense(units=32, activation = 'relu'))
      layer3=model.add(Dense(units=32, activation = 'relu'))
      layer4=model.add(Dense(units=32, activation = 'relu'))
      layer5=model.add(Dense(units=130, activation = 'sigmoid'))
      model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
      classifiers=[model]
      # for layer in model.layers:
      #   weights = layer.get_weights()
      # print(weights)
      for h in range(0,len(classifiers)):
        classifiers[h].fit(training_data.todense(),final_y.toarray(),epochs=20, batch_size = 50)
        prediction= classifiers[h].predict(testing_data)[0] 
      # for layer in model.layers:
      #   weights = layer.get_weights()
      # print(weights)
      return prediction

    # pickle.dump(model,open('test_model.sav','wb'))

    
    # classifiers=[naive_bayes,bagging,KNN,SVM,decision_tree,random_forest,lr,adabost]#,grad_boost]
    # classifiers=[model]
    
    # for i in range(0,len(classifiers_entire_data_set)):
    #   # print('Classifier in use:'+names[i])
    #   classifiers_entire_data_set[i].fit(training_data_entire_data_set.todense(), output_data.toarray(), epochs=20, batch_size = 50)
    # # print(count_vector.get_feature_names())

model.save_weights('my_model_weights.h5')

for z in range(0,100):
      # if z!=4 :
      #   break
      print('Disease Under Consideration:',y_test[z])
      print(z+1)
      obj = cleaning_nlp_processes(x_test[z])
      words = obj.keywords_formation()
      # print('Initially Keywords from symtoms:')
      # print(words)
      for q in range(0,len(keywords_for_matching)):
        # print(y_test[z],keywords_for_matching[q][0],y_test[z].strip()==keywords_for_matching[q][0].strip())
        if y_test[z].strip()==keywords_for_matching[q][0].strip():
          words=words+keywords_for_matching[q][1]
          break
        else:
          continue
      
      # print('Adding additional Keywords From Dataset are:')
      # print(words)
      # flag=1
      # for i in range(0,len(tree)):
      #   if tree[i].entity[len(tree[i].entity)-1]=="'":
      #     # print(tree[i].entity[0:len(tree[i].entity)-2].strip(),data_y[no])
      #     if set(word_tokenize(y_test[z])).issubset(word_tokenize(tree[i].entity[0:len(tree[i].entity)-2])):
      #       print('Ontology Node:',tree[i].entity[0:len(tree[i].entity)-2])
      #       print('Keywords of ontology Node:',tree[i].keywords)
      #       flag=0
      #       break
      #     else:
      #       continue
      #   else:      
      #     # print(tree[i].entity,data_y[no])
      #     if set(word_tokenize(y_test[z])).issubset(word_tokenize(tree[i].entity)):
      #       print('Ontology Node:',tree[i].entity)
      #       print('Keywords of ontology Node:',tree[i].keywords)
      #       flag=0
      #       break
      #     else:
      #       continue
      common_classes=[]
      # if flag==1:
        # print('Disease doesn"t occur in ontology')
        # pass
      possible_classes=ontology_mathing(ontology_tree,words)
      # print('Possibilities After Ontology Matching:',possible_classes)
      for i in range(0,len(all_classes)):
          dataset_elements=[x.lower() for x in list(set(word_tokenize(all_classes[i])))]
          for j in range(0,len(possible_classes)):
            ontology_node=[x.lower() for x in list(set(word_tokenize(possible_classes[j])))]
            if set(dataset_elements).issubset(set(ontology_node)):
              common_classes.append(all_classes[i])
            else:
              continue      
      common_classes=list(set(common_classes))
      # print('Final Possibilites After Ontology Dataset Intersections:',common_classes)
      # print('Number of possible Classes after intersection:',len(common_classes))
      # print('Does Actual Class exsist in Possible Classes after intersection:', y_test[z] in common_classes)
      if len(common_classes)==0:
        c1=c1+1
        # print('No Entry Found in Dataset and Ontology, Using Normal Classification.........')
        # print('Comparing Predictions...............')
        # print('Actual Value:',y_test[z])
        testing_data = count_vector_entire_data_set.transform([x_test[z]])[0]
        prediction=create_model_entire_dataset(training_data_entire_data_set.todense(), output_data.toarray(),testing_data)
        # for h in range(0,len(classifiers_entire_data_set)):
        #   prediction= classifiers_entire_data_set[h].predict(testing_data)[0] 
        predictions[0].append(prediction)
          # print('Comparing Predictions...............')
          # print('Classifier in use: ',names[h])
          # print('Actual value: ',y_test[z])
          # print('Predicted Value:',prediction)  
      else:
        c2=c2+1
        print('Using Ontology Matching............')
        indexes_to_use = []
        for j in range(0, len(y_train)):
          if y_train[j] in common_classes:
            indexes_to_use.append(j)
          else:
            continue
        new_x=[x_train[x] for x in indexes_to_use]
        new_y=[y_train[x] for x in indexes_to_use]
        # print(new_y)
        final_y=onehotencoder.transform((le.transform(new_y)).reshape(-1,1))
        # for i in new_y:
        #   # print('Class:',i)
        #   # print('Label of Class:',le.transform([i]))
        #   obj=(le.transform([i]).reshape(-1,1))
        #   # print('One Hot encoding of Class:', onehotencoder.transform(obj))
        #   final_y.append((obj))
        # # print(final_y)
        count_vector = CountVectorizer(max_features=300)
        training_data = count_vector.fit_transform(new_x)
        testing_data = count_vector.transform([x_test[z]])[0]
        # print(training_data.todense().shape)
        # print(min)
        # min=training_data.shape
        # for h in range(0,len(classifiers)):
        #   for layer in model.layers:
        #     weights = layer.get_weights()
        #   print(weights)
        #   model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
        #   classifiers[h].fit(training_data.todense(),final_y.toarray(),epochs=20, batch_size = 50)
        #   prediction= classifiers[h].predict(testing_data)[0] 
        prediction=create_model(training_data.todense(),final_y.toarray(),testing_data)
        predictions[0].append(prediction)
        # print('Comparing Predictions...............')
        # print('Classifier in use: ',names[h])
        # print('Actual Value:',y_test[z])
        print('Predicted Value: ', str(le.inverse_transform(onehotencoder.inverse_transform([prediction]))))
          # print('Predicted Value:',prediction)
      #   if set(word_tokenize(y_test[z])).issubset(set(word_tokenize(prediction))):
      #     c3=c3+1
      # print(c1,c2,c3)
      print(((z+1)/len(x_test))*100,'% Completed')
      # print(min)
      # print()

from sklearn.metrics import classification_report
print((classification_report(y_test[:len(predictions[0])], predictions[0])))
metrics=[]
for i in range(0,len(predictions)):
  classifier_name=names[i]
  accuracy_score_with_ontology=format(accuracy_score(y_test[:len(predictions[i])], predictions[i]))
  precision_score_with_ontology=format(precision_score(y_test[:len(predictions[i])], predictions[i], average='micro'))
  recall_score_with_ontology=format(recall_score(y_test[:len(predictions[i])], predictions[i], average='micro'))
  f1_score_with_ontology=format(f1_score(y_test[:len(predictions[i])], predictions[i], average='micro'))
  # print('Classifier under consideration:'+ classifier_name)
  print('Accuracy score:', accuracy_score_with_ontology)
  print('Precision score:',precision_score_with_ontology)
  print('Recall score:',recall_score_with_ontology)
  print('F1 score:',f1_score_with_ontology) 
  metrics.append([classifier_name,accuracy_score_with_ontology,precision_score_with_ontology,recall_score_with_ontology,f1_score_with_ontology])

print(len(y_test[:len(predictions[0])]))

from sklearn.metrics import classification_report
predictions_from_ontology=[]
for prediction in predictions[0]:
  prediction=le.inverse_transform(onehotencoder.inverse_transform([prediction]))
  predictions_from_ontology.append(prediction)
# print(predictions_from_ontology)
final_result=[]
for x in predictions_from_ontology:
  for y in x:
    final_result.append(y)
# print(final_result)
print((classification_report(y_test[:len(final_result)], final_result)))

# str(le.inverse_transform(onehotencoder.inverse_transform([prediction])))

for layer in model.layers:
    weights = layer.get_weights()
print(weights)

print(len(x_test))

print(new_x)
count=0
for i in new_x:
  x=len(i.split(' '))
  count+=x
print(training_data.todense().shape)
print(count)

# count_vector = CountVectorizer(max_features=389)
# training_data = count_vector.fit_transform(new_x)
# print(training_data.shape)
p

x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2,random_state=0)
predictions_with_ontology = Classification_with_Ontology_Neural_Network(x_train, y_train,x_test[:500],y_test[:500])
print(predictions_with_ontology)

# from sklearn.metrics import classification_report
# print((classification_report(y_test[:len(predictions_with_ontology)], predictions_with_ontology)))
# from sklearn.metrics import precision_recall_fscore_support
# print(precision_recall_fscore_support(y_test[:len(predictions_with_ontology)], predictions_with_ontology, average='macro'))

metrics=[]
for i in range(0,len(predictions)):
  classifier_name=names[i]
  accuracy_score_with_ontology=format(accuracy_score(y_test[:len(predictions_with_ontology[i])], predictions_with_ontology[i]))
  precision_score_with_ontology=format(precision_score(y_test[:len(predictions_with_ontology[i])], predictions_with_ontology[i], average='micro'))
  recall_score_with_ontology=format(recall_score(y_test[:len(predictions_with_ontology[i])], predictions_with_ontology[i], average='micro'))
  f1_score_with_ontology=format(f1_score(y_test[:len(predictions_with_ontology[i])], predictions_with_ontology[i], average='micro'))
  print('Classifier under consideration:'+ classifier_name)
  print('Accuracy score:', accuracy_score_with_ontology)
  print('Precision score:',precision_score_with_ontology)
  print('Recall score:',recall_score_with_ontology)
  print('F1 score:',f1_score_with_ontology) 
  metrics.append([classifier_name,accuracy_score_with_ontology,precision_score_with_ontology,recall_score_with_ontology,f1_score_with_ontology])
  return metrics

